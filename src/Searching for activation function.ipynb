{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project attempts to implement NIPS 2017 paper \"Searching for activation function\" (Zoph & Le 2017). Although neural networks are powerful and flexible models they are still hard to design and limited to human creativity. One important consideration to designing a neural networks is the activation function as it works as the non-linearity between the affine transformation in a neural network. However how do we choose which basic functions to use and combine with to construct a new activaiton functions. Essentially the problem becomes a search problem of finding the best activation function in a search space. The approach that Zoph and Le took in their paper was similar to their earlier work \"Neural archiecture search using reinforcement learning\" that used a RNN to sample the possible hyperparamers of a neural network while using a polciy gradient approach(specifically REINFORCE)to train the network to maximise the validation accuracy of the child network (i.e. the reward signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/nas.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper they used a RNN to instead generate the choice of unary and binary functions used to create the activation function and trained the RNN using the Policy Proximal Optimization (PPO) algorthim.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/activationfunctions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PPO is in strak contrast with the other 2 papers previous publish by Zoph & Le which used REINFORCE and Trust Region Policies (TRP). A reason why did not used REINFORCE was given in their paper \"Neural Optimizer Search with Reinforcement Learning\", there the authors explanation was that REINFORCE exhbited poor sampling efficiency comapred to Trust Region Policies. However no explanations was given as to why PPO was used in searching for activations instead of TRP. Another thing the author failed to explain was why they used policy gradient methods instead of other approaches like evolutionary strategies. Finally there was no control done in any of the papers as to whether their approach was better than say random search and how much better was it than random search. Such a controlled experiment might also make it possible to compare other approaches say evolutionary strategies with their method more fairly. This is however outside the scope of a mere paper implementation and is worth considering as a future project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start searching for activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download cifar-10 datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python cifar10_download_and_extract.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search is conducted using ResNet-20 as the child network architecture and trained on CIFAR-10 for 10k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper 3 datasets were used to test the transfer capabilties of the newly found activation functions \n",
    "1. CIFAR-100\n",
    "2. ImageNet\n",
    "3. WMT\n",
    "\n",
    "I wasn't able to donwload imagenet because of it's large size. WMT 2014 EnglishIn this notebook we only look at CIFAR100 and WMT 2014 English-German Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python cifar100_download_and_extract.py\n",
    "!python cifar100_train.py\n",
    "!python cifar100_test.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
